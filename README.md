# Background Information
As one of the data scientists in an American public transport company, the logistic team ask you for help. They want to optimize the usage of the company's resources (the vehicles/taxis) by covering places with high demand for public transport. You should help them to predict the most important clusters for public transport (using taxis) as well as their progression over the day. To this end the logistic team gives you a dataset of the GPS data and starting time of the company's vehicles over the last six months. Use this data to create a regression model, possibly multiple ones, to predict the average demand of taxis per hour over the day in the ten most important clusters in the city. Discuss possible demand variability and propose a suitable prediction approach.
###### Task 
Support the logistic team in the company with a prediction approach for the average demand of taxis per hour over the day in the ten most important clusters of the available data.
### 1. Business Understanding
We have a taxi company that wants to use prediction algorithms to improve the company's resource usage, i.e., have more taxis driving people and less taxis waiting for people. Therefore, we want to understand when and where people usually need and don't need taxis to position/deploy the taxis more effectively. This is not only an improvement for the business itself, but also for the customers as their needs will be better met.
### 2. Data Understanding
The data is not very extensive, but sufficient. There are individual data sets from April 2014 until September 2014. The data includes coordinates and timestamps, the "Base" column is essentially useless, as it only repeats the same value and provides no real information. Therefore, all information available is where and when taxis are used. Unfortunately, there is no information regarding if a customer called a taxi to a given location or just entered it upon sight. Also, it is unclear how many taxis have been deployed at certain areas during certain times or how long a taxi has been occupied. Therefore, the actual demand is unclear as the data might be biased due to induced demand, i.e., more taxis lead to more people using taxis. For sake of the task, this will be ignored.
Upon exploring the data, it is very apparent that the overall demand, regardless of position, follows a clear pattern and could most likely be modeled using harmonic regression. The data seems to be quite clean.
### 3. Data Preparation
###### Turning CSV into SQL
For many reasons, the .csv files will be combined into one large SQL Database using the sqlite3 library provided by Python. A few of the reasons are better latency and much easier working given that working with an SQL database is much more superior to working with several .csv files. It will also come in handy when retrieving specific subsets of the data. During this process, **all null values were removed**.
Visual exploration did show a few outliers, but not to a degree where they would be unreasonable/unrealistic. Given that a clustering algorithm will be used, where outliers will simply be grouped to the nearest cluster node, outliers will be kept.
###### Splitting Date Column
There is a reasonable assumption that there might be daily patterns in taxi demands, such as at the start or end of workdays. To better analyze this, the "Date/Time" column will be split up into a "date" and "time" column.
### 4. Model Engineering
As the task asks for the 10 most important clusters, the clustering algorithm will be used to create 20 clusters. During iteration, 20 clusters have been shown to be a reasonable amount, as there is a high concentration of clusters around the center of the city. Using a KNN clustering algorithm, all data points will be grouped and 20 centroids will be determined, which can be used for classifying future data points.
### 5. Model Evaluation
As simply the average will be used, there is no much use in measuring the model's performance. There is also not much use to measuring the performance of a clustering algorithm in a case like this, so no evaluation will be made.
The final values given by `predictor.py` should be used as reference values. The predictions could be made more complex, using statistical models such as SARIMA, or general machine learning models such as ANNs. However, since this task is of rather simple nature, a clustering algorithm and average values is deemed sufficient, as the task does ask specifically for the average "predict the average demand of taxis per hour over the day in the ten most important clusters", so we will assume the system is static.